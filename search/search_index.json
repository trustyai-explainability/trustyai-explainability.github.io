{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-trustyai","title":"Welcome to TrustyAI","text":"<p>TrustyAI is an open source AI Explainability (XAI) Toolkit. It provides a Java and Python library and Open Data Hub (ODH) component for XAI explanations of predictive models for both enterprise and data science.</p> <p>TrustyAI is avalable as a Python library and a Java library and as a Open Data Hub (ODH) component.</p> <p>TrustyAI provides a set of explainability algorithms and a set of bias and fairness metrics. The explainability algorithms are used to generate explanations for black-box predictive models. The metrics are used to evaluate potential bias in predictive models.</p>"},{"location":"#explainers","title":"Explainers","text":"<p>The explainers provided are explained in more detail in the explainers overview section.</p> <p>The metrics provided are explained in more detail in the metrics overview section.</p>"},{"location":"#quickstarts","title":"Quickstarts","text":"<p>To get started with the TrustyAI service (the ODH component), see the TrustyAI service quickstart.</p>"},{"location":"algorithms/explainers/","title":"Overview","text":"<p>TrustyAI service supports a number of explainers.</p>"},{"location":"algorithms/explainers/local/lime/","title":"LIME","text":""},{"location":"algorithms/metrics/","title":"Overview","text":""},{"location":"algorithms/metrics/#demographic-parity","title":"Demographic parity","text":"<ul> <li>Statistical Parity Difference (SPD)</li> <li>Disparate impact ratio (DIR)</li> </ul>"},{"location":"algorithms/metrics/dir/","title":"Disparate impact ratio","text":"<p>Similarly to the Statistical Parity Difference, the Disparate Impact Ratio (DIR) measures imbalances in positive outcome predictions across privileged and unprivileged groups. Instead of calculating the difference, this metric calculates the ratio of such selection rates. Typically:</p> <ul> <li>$DIR=1$ means that the model is fair with regards to the protected attribute.</li> <li>$0.8&lt;DIR&lt;1.2$ means that the model is reasonably fair.</li> </ul> <p>The formal definition of the Disparate Impact Ratio is:</p> <p>$$ DIR=\\frac{p(\\hat{y}=1|D_u)}{p(\\hat{y}=1|D_p)} $$</p>"},{"location":"algorithms/metrics/spd/","title":"Statistical Parity Difference","text":"<p>The Statistical Parity Difference (SPD)  is the difference in the probability of prediction between the privileged and unprivileged groups. Typically:</p> <ul> <li>$SPD=0$ means that the model is behaving fairly in regards of the selected attribute (e.g. race, gender)</li> <li>Values between $-0.1&lt;SPD&lt;0.1$ mean that the model is reasonably fair and the score can be attributed to other factors, such as sample size.</li> <li>An SPD outside this range would be an indicator of an unfair model relative to the protected attributes.</li> <li>A negative value of statistical parity difference indicates that the unprivileged group is at a disadvantage</li> <li>A positive value indicates that the privileged group is at a disadvantage.</li> </ul> <p>The formal definition of SPD is</p> <p>$$ SPD=p(\\hat{y}=1|D_u)-p(\\hat{y}=1|D_p) $$</p> <p>where $\\hat{y}=1$ is the favourable outcome and $D_u$, $D_p$ are respectively the privileged and unprivileged group data.</p>"},{"location":"python/","title":"Introduction","text":""},{"location":"service/","title":"Introduction","text":"<p>The TrustyAI service.</p>"},{"location":"service/#explainers","title":"Explainers","text":"<p>The TrustyAI service supports a number of explainers.</p>"},{"location":"service/#time-series","title":"Time-series","text":"<ul> <li>TSSaliency</li> </ul> <p>The TrustyAI service includes several demos.</p> <ul> <li>generating data into storage, which can be monitored by the service</li> <li>or, having a process simulating sending KServe gRPC data to a consumer endpoint</li> </ul> <p>With either of these demos, the TrustyAI service will monitor the payloads and produce fairness metrics.</p> <p>The first step to run the demos locally, is to build the TrustyAI service container image. This can be done by running (on <code>$PROJECT/explainability-service</code>):</p> <pre><code>mvn clean install -Dquarkus.container-image.build=true\n</code></pre>"},{"location":"service/#using-data-in-storage-only","title":"Using data in storage only","text":"<p>To run this demo, first build the remaining images using:</p> <pre><code>$ cd demo\n$ docker compose -f compose-generator-memory-single-model.yaml build\n</code></pre> <p>Finally, run the demo using:</p> <pre><code>$ docker compose -f compose-generator-memory-single-model.yaml up\n</code></pre> <p>Issue a metric request to, for instance:</p> <pre><code>curl -X POST --location \"http://localhost:8080/metrics/spd/request\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n            \\\"modelId\\\": \\\"example-model-1\\\",\n            \\\"requestName\\\": \\\"lala\\\",\n            \\\"protectedAttribute\\\": \\\"input-2\\\",\n            \\\"favorableOutcome\\\":  1.0,\n            \\\"outcomeName\\\": \\\"output-0\\\",\n            \\\"privilegedAttribute\\\": 1.0,\n            \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre> <p>And observe the <code>trustyai_spd</code> metric in Prometheus: http://localhost:9090</p>"},{"location":"service/#own-data-in-minio","title":"Own data in MinIO","text":"<p>To use your own provided data, configure the MinIO container (by pre-populating it with data, according to the steps below) and run the container using (either <code>docker</code>, <code>podman</code>):</p> <pre><code>docker run -p 8080:8080 \\\n--env SERVICE_DATA_FORMAT=CSV \\\n--env SERVICE_STORAGE_FORMAT=\"MINIO\" \\\n--env MINIO_BUCKET_NAME=\"inputs\" \\\n--env MINIO_ENDPOINT=\"http://localhost:9000\" \\\n--env STORAGE_DATA_FILENAME=\"income-biased-inputs.csv\" \\\n--env STORAGE_DATA_FOLDER=\"/inputs\" \\\n--env MINIO_SECRET_KEY=\"minioadmin\" \\\n--env MINIO_ACCESS_KEY=\"minioadmin\" \\\n--env SERVICE_METRICS_SCHEDULE=\"5s\" \\\ntrustyai/trustyai-service:999-SNAPSHOT -d </code></pre> <p>When using the metrics HTTP request, remember to adjust the feature names and values to what makes sense for your own data.</p>"},{"location":"service/#consuming-kserve-v2-data","title":"Consuming KServe v2 data","text":"<p>Another demo includes a process with simulates sending gRPC encoded KServe v2 data to a <code>consumer</code> endpoint in the service. The service then parses the data and saves it into storage.</p> <p>To run it, start by building the necessary images with:</p> <pre><code>$ cd demo\n$ docker compose -f compose-generator-pvc-multi-model.yaml build\n</code></pre> <p>This demo uses a Docker bind mount, which on the host can be created with:</p> <pre><code>mkdir -p ~/volumes/pvc/inputs\n</code></pre> <p>Note:</p> <p>If you are having permission errors from the service, while saving the data to the volume, change the permissions with <code>cmhod 777 ~/volume/pvc/inputs</code></p> <p>The demo can then be started with:</p> <pre><code>docker compose -f compose-generator-pvc-multi-model.yaml.yml up\n</code></pre> <p>After a few seconds, you will start seeing the logs showing both the payload sent</p> <pre><code>generator         | Sending data\ntrustyai-service  | 2023-02-18 12:22:13,572 INFO  [org.kie.tru.ser.end.con.ConsumerEndpoint] (executor-thread-1) Got payload on the consumer\ntrustyai-service  | 2023-02-18 12:22:13,572 INFO  [org.kie.tru.ser.end.con.ConsumerEndpoint] (executor-thread-1) [Feature{name='input-0', type=number, value=22.0}, Feature{name='input-1', type=number, value=5.0}, Feature{name='input-2', type=number, value=1.0}]\ntrustyai-service  | 2023-02-18 12:22:13,572 INFO  [org.kie.tru.ser.end.con.ConsumerEndpoint] (executor-thread-1) [Output{value=1.0, type=number, score=1.0, name='output-0'}]\ntrustyai-service  | 2023-02-18 12:22:18,001 INFO  [org.kie.tru.ser.dat.par.CSVParser] (executor-thread-1) Creating dataframe from CSV data\ntrustyai-service  | 2023-02-18 12:22:18,001 INFO  [org.kie.tru.ser.dat.DataSource] (executor-thread-1) Batching with 5000 rows. Passing 73 rows\n</code></pre> <p>You can also inspect the data <code>~/volumes/pvc/inputs</code> in order to see what data is being serialised.</p>"},{"location":"service/#s3-minio","title":"S3 (MinIO)","text":"<p>In order to set up MinIO for local development, first install the MinIO client <code>mc</code>. Run the MinIO server with</p> <pre><code>docker run \\\n-p 9000:9000 \\\n-p 9090:9090 \\\n--name minio \\\n-v ~/minio/trustyai-service/data:/data \\\n-e \"MINIO_ROOT_USER=minioadmin\" \\\n-e \"MINIO_ROOT_PASSWORD=minioadmin\" \\\nquay.io/minio/minio server /data --console-address \":9090\"\n</code></pre> <p>Connect to MinIO using:</p> <pre><code>mc alias set local http://127.0.0.1:9000 minioadmin minioadmin\n</code></pre> <p>Now create a bucket, for instance, <code>inputs</code>:</p> <pre><code>mc mb local/inputs\n</code></pre> <p>Copy a file into the bucket:</p> <pre><code>mc cp data/income-biased-inputs.csv local/inputs\n</code></pre> <p>Optionally, check the file was successfully copies:</p> <pre><code>mc ls local/inputs\n</code></pre> <p>Which should produce:</p> <pre><code>[2023-02-09 23:01:49 GMT]  68KiB income-biased-inputs.csv\n</code></pre>"},{"location":"service/#endpoints","title":"Endpoints","text":"<p>The OpenAPI schema can be displayed using</p> <pre><code>curl -X GET --location \"http://localhost:8080/q/openapi\"\n</code></pre>"},{"location":"service/#metrics","title":"Metrics","text":"<p>Each of the metrics default bounds can be overridden with the corresponding environment variable, e.g.</p> <ul> <li><code>METRICS_SPD_THRESHOLD_LOWER</code></li> <li><code>METRICS_SPD_THRESHOLD_UPPER</code></li> <li><code>METRICS_DIR_THRESHOLD_LOWER</code></li> <li>etc</li> </ul>"},{"location":"service/#statistical-parity-difference","title":"Statistical Parity Difference","text":"<p>Get statistical parity difference at <code>/metrics/spd</code></p> <pre><code>curl -X POST --location \"http://{{host}}/metrics/spd\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0,\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre> <p>Returns:</p> <p>```http request HTTP/1.1 200 OK content-length: 199 Content-Type: application/json;charset=UTF-8</p> <p>{   \"type\": \"metric\",   \"name\": \"SPD\",   \"value\": -0.2531969309462916,   \"specificDefinition\":\"The SPD of -0.253196 indicates that the likelihood of Group:gender=1 receiving Outcome:income=1 was -25.3196 percentage points lower than that of Group:gender=0.\"   \"timestamp\": 1675850601910,   \"thresholds\": {     \"lowerBound\": -0.1,     \"upperBound\": 0.1,     \"outsideBounds\": true   },   \"id\": \"ec435fc6-d037-493b-9efc-4931138d7656\" } <pre><code>### Disparate Impact Ratio\n\n```shell\ncurl -X POST --location \"http://{{host}}/metrics/dir\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0,\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre></p> <p>```http request HTTP/1.1 200 OK content-length: 197 Content-Type: application/json;charset=UTF-8 {   \"type\": \"metric\",   \"name\": \"DIR\",   \"value\": 0.3333333333333333,   \"specificDefinition\":\"The DIR of 0.33333 indicates that the likelihood of Group:gender=1 receiving Outcome:income=1 is 0.33333 times that of Group:gender=0.\"   \"id\": \"15f87802-30ae-424b-9937-1589489d6b4b\",   \"timestamp\": 1675850775317,   \"thresholds\": {     \"lowerBound\": 0.8,     \"upperBound\": 1.2,     \"outsideBounds\": true   } } <pre><code>### Scheduled metrics\n\nIn order to generate period measurements for a certain metric, you can send a request to\nthe `/metrics/$METRIC/schedule`.\nLooking at the SPD example above if we wanted the metric to be calculated periodically we would request:\n\n```shell\ncurl -X POST --location \"http://{{host}}/metrics/spd/request\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre></p> <p>We would get a response with the schedule id for this specific query:</p> <p>```http request HTTP/1.1 200 OK content-length: 78 Content-Type: application/json;charset=UTF-8</p> <p>{   \"requestId\": \"3281c891-e2a5-4eb3-b05d-7f3831acbb56\",   \"timestamp\": 1676031994868 } <pre><code>The metrics will now be pushed to Prometheus with the runtime provided `SERVICE_METRICS_SCHEDULE` configuration (\ne.g. `SERVICE_METRICS_SCHEDULE=10s`)\nwhich follows the [Quarkus syntax](https://quarkus.io/guides/scheduler-reference).\n\nYou can also specify the bias threshold deltas in the request body:\n\n```shell\ncurl -X POST --location \"http://{{host}}/metrics/spd/request\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"thresholdDelta\\\": 0.05,\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0,\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre></p> <p>This means that this specific metric request will consider SPD values within +/-0.05 to be fair, and values outside those bounds to be unfair.</p> <p>You can also specify the batch size in the request body:</p> <pre><code>curl -X POST --location \"http://{{host}}/metrics/spd/request\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"batchSize\\\": 1000,\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0,\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre> <p>This mean that for this specific metric request the dataset used will consist of the last 1000 records. If the batch size is omitted, the default value is taken from the configuration variable <code>BATCH_SIZE</code> as the default.</p> <p>To stop the periodic calculation you can issue an HTTP <code>DELETE</code> request to the <code>/metrics/$METRIC/request</code> endpoint, with the id of periodic task we want to cancel in the payload. For instance:</p> <pre><code>curl -X DELETE --location \"http://{{host}}:8080/metrics/spd/request\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"requestId\\\": \\\"3281c891-e2a5-4eb3-b05d-7f3831acbb56\\\"\n        }\"\n</code></pre> <p>To list all currently active requests for a certain metric, use <code>GET /metrics/{{metric}}/requests</code>. For instance, to get all current scheduled SPD metrics use:</p> <pre><code>curl -X GET --location \"http://{{host}}:8080/metrics/spd/requests\"\n</code></pre> <p>This will return, as an example:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: application/json;charset=UTF-8\ncontent-length: 271\n{\n\"requests\": [\n{\n\"id\": \"ab46d639-6567-438b-a0aa-44ee9fd423a3\",\n      \"request\": {\n\"protectedAttribute\": \"input-2\",\n        \"favorableOutcome\": {\n\"type\": \"DOUBLE\",\n          \"value\": 1.0\n      },\n      \"outcomeName\": \"output-0\",\n      \"privilegedAttribute\": {\n\"type\": \"DOUBLE\",\n        \"value\": 1.0\n      },\n      \"unprivilegedAttribute\": {\n\"type\": \"DOUBLE\",\n      \"value\": 0.0\n      },\n      \"modelId\": null\n    }\n}\n]\n}\n</code></pre>"},{"location":"service/#metric-definitions","title":"Metric Definitions","text":"<p>To get a general definition of a metric, you can issue an HTTP <code>GET</code> request to the <code>/metrics/$METRIC/definition</code> endpoint:</p> <pre><code>curl -X GET http://{{host}}:8080/metrics/{{metric}}/definition\n</code></pre> <p>returns</p> <pre><code>Statistical Parity Difference (SPD) measures imbalances in classifications by calculating the difference between the proportion of the majority and protected classes getting a particular outcome. Typically, -0.1 &lt; SPD &lt; 0.1 indicates a fair model, while a value outside those bounds indicates an unfair model for the groups and outcomes in question\"\n</code></pre> <p>To get a specific definition of what a particular value means in the context of a specific computed metric, you can issue an HTTP <code>POST</code> request to the <code>/metrics/$METRIC/definition</code> endpoint. The body of this request will look identical to a normal metric request, except you will specify the metric value of interest within the <code>metricValue</code> field. This is equivalent to asking \"If I computed this metric in this configuration, what would a value of $x mean?\":</p> <pre><code>curl -X POST --location \"http://{{host}}:8080/metrics/{metric}/definition\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"protectedAttribute\\\": \\\"gender\\\",\n          \\\"favorableOutcome\\\": 1\n          \\\"outcomeName\\\": \\\"income\\\",\n          \\\"privilegedAttribute\\\": 1\n          \\\"unprivilegedAttribute\\\": 0\n          \\\"metricValue\\\": 0.25\n        }\"\n</code></pre> <p>returns</p> <pre><code>The SPD of 0.250000 indicates that the likelihood of Group:gender=1 receiving Outcome:income=1 was 25.000000 percentage points higher than that of Group:gender=0.%\n</code></pre>"},{"location":"service/#explainers_1","title":"Explainers","text":"<p>The TrustyAI service provides local and globalsexplainers. The supported explainers are:</p> <ul> <li>LIME</li> <li>SHAP</li> <li>Counterfactuals</li> <li>TSSaliency</li> <li>PDP</li> </ul>"},{"location":"service/#tssaliency","title":"TSSaliency","text":"<p>Assuming there's a ModelMesh-deployed model at host <code>$MODELSERVER</code> and using a standard <code>8081</code> port, with name <code>$MODELNAME</code> and version <code>$MODELVERSION</code>, a TSSaliency explainer can be invoked with:</p> <pre><code>curl -X POST --location \"http://{{host}}:8080/explainers/local/tssaliency\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"model\\\": {\n            \\\"target\\\": \"$MODELHOST:8081\",\n            \\\"name\\\": $MODELNAME,\n            \\\"version\\\": $MODELVERSION\n          },\n          \\\"parameters\\\": {\n            \\\"numberSamples\\\": 100,\n            \\\"numberSteps\\\": 50,\n            \\\"sigma\\\": 20.0,\n            \\\"mu\\\": 0.1\n          },\n          \\\"data\\\": {\n            \\\"f1\\\": [\n              -0.14040239,\n              0.17164128,\n              0.30204415,\n              0.23280369,\n              0.033852769,\n              -0.22418335,\n              -0.46998698,\n              -0.64539614,\n              -0.61769196\n            ]\n          }\n      }\"\n</code></pre> <p>Where <code>f1</code> is the time-series instance to be explained. The <code>parameters</code> section is optional and can be omitted, in which case the default values will be used.</p>"},{"location":"service/#prometheus","title":"Prometheus","text":"<p>Whenever a metric endpoint is called with a HTTP request, the service also updates the corresponding Prometheus metric.</p> <p>The metrics are published at <code>/q/metrics</code> and can be consumed directly with Prometheus. The examples also include a Grafana dashboard to visualize them.</p> <p></p> <p>Each Prometheus metric is scoped to a specific <code>model</code> and attributes using tags. For instance, for the SPD metric request above we would have a metric:</p> <pre><code>trustyai_spd{\n    favorable_value=\"1\", \n    instance=\"trustyai:8080\", \n    job=\"trustyai-service\", \n    model=\"example\", \n    outcome=\"income\", \n    privileged=\"1\", \n    protected=\"gender\", \n    request=\"e4bf1430-cc33-48a0-97ce-4d0c8b2c91f0\", \n    unprivileged=\"0\"\n}\n</code></pre>"},{"location":"service/#health-checks","title":"Health checks","text":"<p>The service provides an health check endpoint at <code>/q/health</code>:</p> <pre><code>curl {{host}}:8080/q/health\n</code></pre>"},{"location":"service/#consuming-kserve-v2-payloads","title":"Consuming KServe v2 payloads","text":"<p>The TrustyAI service provides an endpoint to consume KServe v2 inference payloads. When received, these will be persisted to the configured storage and used, for instance, in the calculation of metrics.</p> <p>The payload consists of a JSON object with an <code>input</code> and <code>output</code> fields, which contain the Base64 encoded raw bytes of the gRPC Protocol payload. As an example:</p> <pre><code>curl -X POST --location \"http://{{host}}:8080/consumer/kserve/v2\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n  \\\"modelId\\\": \\\"example-2\\\",\n  \\\"input\\\": \\\"CgdleGFtcGxlGg1teSByZXF1ZXN0IGlkKiUKBWlucHV0EgRGUDY0GgIBAyoSOhAAAAAAAABUQAAAAAAAABBA\\\",\n  \\\"output\\\": \\\"CgdleGFtcGxlGg1teSByZXF1ZXN0IGlkKh0KBWlucHV0EgRGUDY0GgIBASoKOggAAAAAAAAAAA==\\\"\n}\"\n</code></pre>"},{"location":"service/#service-info","title":"Service info","text":"<p>To retrieve service info, you can issue a <code>GET /info</code>. This is will return currently registered scheduled metrics, number of observations in the dataset and dataset schema.</p> <pre><code>curl -X GET --location \"http://{{host}}:8080/info\"\n</code></pre> <p>Will return, for instance</p> <pre><code>[\n    {\n        \"metrics\": {\n            \"scheduledMetadata\": {\n                \"spd\": 1,\n                \"dir\": 0\n            }\n        },\n        \"data\": {\n            \"inputSchema\": {\n                \"items\": {\n                    \"input-0\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-0\",\n                        \"index\": 1\n                    }, \n                },\n                nameMapping: {\n                    \"input-0\": \"Age\",\n                }\n            },\n            \"outputSchema\": {\n                \"items\": {\n                    \"output-0\": {\n                        \"type\": \"INT32\",\n                        \"name\": \"output-0\",\n                        \"index\": 5\n                    }\n                },\n                \"nameMapping:\" {\n                    \"output-0\": \"Income\",\n                }\n            },\n            \"observations\": 105,\n            \"modelId\": \"example-model-1\"\n        }\n    },\n    {\n        \"metrics\": {\n            \"scheduledMetadata\": {\n                \"spd\": 1,\n                \"dir\": 0\n            }\n        },\n        \"data\": {\n            \"inputSchema\": {\n                \"items\": {\n                    \"input-0\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-0\",\n                        \"index\": 1\n                    },\n                     \"input-1\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-1\",\n                        \"index\": 2\n                    }              \n                },\n                nameMapping: {\n                    \"input-0\": \"Age\",\n                    \"input-2\": \"Race\",\n                }\n            },\n            \"outputSchema\": {\n                \"items\": {\n                    \"output-0\": {\n                        \"type\": \"INT32\",\n                        \"name\": \"output-0\",\n                        \"index\": 5\n                    }\n                },\n                \"nameMapping:\" {\n                    \"output-0\": \"Income\",\n                }\n            },\n            \"observations\": 105,\n            \"modelId\": \"example-model-2\"\n        }\n    },\n    {\n        \"metrics\": {\n            \"scheduledMetadata\": {\n                \"spd\": 1,\n                \"dir\": 0\n            }\n        },\n        \"data\": {\n            \"inputSchema\": {\n                \"items\": {\n                    \"input-0\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-0\",\n                        \"index\": 1\n                    },\n                     \"input-1\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-1\",\n                        \"index\": 2\n                    },\n                    \"input-2\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-1\",\n                        \"index\": 3\n                    },\n                    \"input-3\": {\n                        \"type\": \"DOUBLE\",\n                        \"name\": \"input-3\",\n                        \"index\": 4\n                    }\n                },\n                nameMapping: {\n                    \"input-0\": \"Age\",\n                    \"input-2\": \"Race\",\n                    \"input-3\": \"Gender\",\n                    \"input-4\": \"Employment\",\n                }\n            },\n            \"outputSchema\": {\n                \"items\": {\n                    \"output-0\": {\n                        \"type\": \"INT32\",\n                        \"name\": \"output-0\",\n                        \"index\": 5\n                    },\n                    \"output-1\": {\n                            \"type\": \"INT32\",\n                            \"name\": \"output-1\",\n                            \"index\": 6\n                    }\n                },\n                \"nameMapping:\" {\n                    \"output-0\": \"Income\",\n                    \"output-1\": \"Credit Score\",\n                }\n            },\n            \"observations\": 85,\n            \"modelId\": \"example-model-3\"\n        }\n    }\n]\n</code></pre>"},{"location":"service/#defining-featureoutput-names","title":"Defining Feature/Output Names","text":"<pre><code>curl -X POST --location \"http://localhost:8080/q/info\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n        \"modelId\": \"example-model-1\", \n        \"inputMapping\": \n            {\n                \"input-0\": \"age\", \n                \"input-1\": \"race\",\n                \"input-2\": \"gender\"\n            },\n        \"outputMapping\": \n            {\n                \"output-0\": \"predictedIncome=high\"\n            }\n    }'\n</code></pre>"},{"location":"service/#data-sources","title":"Data sources","text":""},{"location":"service/#metrics_1","title":"Metrics","text":"<p>Storage backend adapters implement the <code>Storage</code> interface which has the responsibility of reading the data from a specific storage type (flat file on PVC, S3, database, etc) and return the inputs and outputs as <code>ByteBuffer</code>. From there, the service converts the <code>ByteBuffer</code> into a TrustyAI <code>Dataframe</code> to be used in the metrics calculations.</p> <p>The type of datasource is passed with the environment variable <code>SERVICE_STORAGE_FORMAT</code>.</p> <p>The supported data sources are:</p> Type Storage property MinIO <code>MINIO</code> Kubernetes Persistent Volume Claims (PVC) <code>PVC</code> Memory <code>MEMORY</code> <p>The data can be batched into the latest <code>n</code> observations by using the configuration key <code>SERVICE_BATCH_SIZE=n</code>. This behaves like a <code>n</code>-size tail and its optional. If not specified, the entire dataset is used.</p>"},{"location":"service/#deployment","title":"Deployment","text":""},{"location":"service/#openshift","title":"OpenShift","text":"<p>To deploy in Kubernetes or OpenShift, the connection information can be passed into the manifest using the <code>ConfigMap</code> in here.</p> <p>The main manifest is available here.</p> <p>The configuration variables include:</p> Environment variable Values Default Purpose <code>QUARKUS_CACHE_ENABLED</code> <code>true</code>/<code>false</code> <code>true</code> Enables data fetching and metric calculation caching. Enabled by default."},{"location":"service/quickstart/","title":"Quickstart","text":""},{"location":"service/running/","title":"Running","text":""},{"location":"service/running/#locally","title":"Locally","text":"<p>The TrustyAI service is at its core a Quarkus application, which can be run locally using:</p> <pre><code>$ mvn quarkus:dev\n</code></pre> <p>This will start the service on port <code>8080</code>, using a in-memory database.</p>"},{"location":"service/running/#demos","title":"Demos","text":"<p>The TrustyAI service includes several demos.</p> <ul> <li>generating data into storage, which can be monitored by the service</li> <li>or, having a process simulating sending KServe gRPC data to a consumer endpoint</li> </ul> <p>With either of these demos, the TrustyAI service will monitor the payloads and produce fairness metrics.</p> <p>The first step to run the demos locally, is to build the TrustyAI service container image. This can be done by running (on <code>$PROJECT/explainability-service</code>):</p> <pre><code>mvn clean install -Dquarkus.container-image.build=true\n</code></pre>"},{"location":"service/running/#using-data-in-storage-only","title":"Using data in storage only","text":"<p>To run this demo, first build the remaining images using:</p> <pre><code>$ cd demo\n$ docker compose -f compose-generator-memory-single-model.yaml build\n</code></pre> <p>Finally, run the demo using:</p> <pre><code>$ docker compose -f compose-generator-memory-single-model.yaml up\n</code></pre>"},{"location":"service/explainers/time-series/tssaliency/","title":"TSSaliency","text":"<p>TSSaliency is a time-series explainer for black-box forecaster models.</p> <p>The endpoint is <code>/explainers/local/tssaliency</code>.</p> <p>The payload follows the time-series general schema with the following parameters:</p> <pre><code>{\n\"model\": {\n\"target\": \"0.0.0.0:8081\", // (1)!\n\"name\": \"tsforda\",\n\"version\": \"v0.1.0\"\n},\n\"data\": {\n\"x\": [23.45, 25.78, 24.32, 26.11, 25.92, 23.67, 24.53, 25.99, 26.24, 24.86]\n},\n\"parameters\": { // (2)!\n\"mu\": 0.01,\n\"numberSamples\": 50,\n\"numberSteps\": 10,\n\"sigma\": 10.0\n}\n}\n</code></pre> <ol> <li>This will consist of the address of the model. Typically <code>8081</code> is the gRPC port of the model, which is needed.</li> <li>All the parameters are optional. If not provided, a default values will be used.</li> </ol> <p>The following values are mandatory:</p> <ul> <li><code>target</code>: The address of the model. TrustyAI communicates with the model through gRPC, so the address should be in the form of <code>host:port</code> with port being typically <code>8081</code>.</li> <li><code>name</code>: The name of the model. This is used to identify the model in TrustyAI.</li> <li><code>version</code>: The version of the model. This is used to identify the model in TrustyAI.</li> <li><code>data</code>: The time-series to be explained. Only numerical list supported at the moment. The data is structure as a list of features with each entry corresponding to a time-step.</li> </ul> <p>All the parameters are optional. If not provided, a default values will be used.</p> <p>The result will be a saliency map for the input time-series.</p>"},{"location":"service/metrics/","title":"Overview","text":"<pre><code>sequenceDiagram\n    participant R as Requester\n    participant MM as ModelMesh\n    participant TAI as TrustyAI Service\n    participant DB as Database\n    participant P as Prometheus\n\n    R-&gt;&gt;MM: Inference Request\n    activate TAI\n    MM-&gt;&gt;TAI: Forward Request\n    MM--&gt;&gt;TAI: Response\n    deactivate TAI\n    MM--&gt;&gt;R: Response\n    TAI-&gt;&gt;TAI: Reconcile Response\n    TAI-&gt;&gt;DB: Persist inference data\n    loop Periodically\n        TAI-&gt;&gt;P: Send to Prometheus\n    end</code></pre>"},{"location":"service/metrics/dir/","title":"Disparate Impact Ratio","text":"<pre><code>curl -X POST --location \"http://{{host}}/metrics/dir\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0,\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre> <pre><code>HTTP/1.1 200 OK\ncontent-length: 197\nContent-Type: application/json;charset=UTF-8\n{\n\"type\": \"metric\",\n\"name\": \"DIR\",\n\"value\": 0.3333333333333333,\n\"specificDefinition\":\"The DIR of 0.33333 indicates that the likelihood of Group:gender=1 receiving Outcome:income=1 is 0.33333 times that of Group:gender=0.\"\n\"id\": \"15f87802-30ae-424b-9937-1589489d6b4b\",\n\"timestamp\": 1675850775317,\n\"thresholds\": {\n\"lowerBound\": 0.8,\n\"upperBound\": 1.2,\n\"outsideBounds\": true\n}\n}\n</code></pre>"},{"location":"service/metrics/spd/","title":"Statistical Parity Difference","text":"<p>Get statistical parity difference at <code>/metrics/spd</code></p> <pre><code>curl -X POST --location \"http://{{host}}/metrics/spd\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{\n          \\\"modelId\\\": \\\"example-model-1\\\",\n          \\\"protectedAttribute\\\": \\\"input-2\\\",\n          \\\"favorableOutcome\\\": 1.0,\n          \\\"outcomeName\\\": \\\"output-0\\\",\n          \\\"privilegedAttribute\\\": 1.0,\n          \\\"unprivilegedAttribute\\\": 0.0\n        }\"\n</code></pre> <p>Returns:</p> <pre><code>HTTP/1.1 200 OK\ncontent-length: 199\nContent-Type: application/json;charset=UTF-8\n{\n\"type\": \"metric\",\n\"name\": \"SPD\",\n\"value\": -0.2531969309462916,\n\"specificDefinition\":\"The SPD of -0.253196 indicates that the likelihood of Group:gender=1 receiving Outcome:income=1 was -25.3196 percentage points lower than that of Group:gender=0.\"\n\"timestamp\": 1675850601910,\n\"thresholds\": {\n\"lowerBound\": -0.1,\n\"upperBound\": 0.1,\n\"outsideBounds\": true\n},\n\"id\": \"ec435fc6-d037-493b-9efc-4931138d7656\"\n}\n</code></pre>"}]}